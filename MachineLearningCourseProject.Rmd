---
title: "Predicting Exercise Performance Using Accelerometer Data"
author: "Richard A. Lent"
header-includes: \usepackage{caption}
output:
  html_notebook:
    theme: yeti
  html_document:
    self_contained: no
    theme: yeti
editor_options:
  chunk_output_type: inline
---

```{r Global chunk options, include=FALSE}

# These options apply to all subsequent R code chunks.
knitr::opts_chunk$set(
	echo = TRUE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	comment = NA
)

```

**
`r format(Sys.time(), '%e %B %Y')`
**

This project uses data^[See [Human Activity Recognition](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har).] from accelerometers worn on the belt, forearm, arm, and barbell of six participants, who were asked to perform barbell lifts correctly and incorrectly in five different ways. The goal is to predict the manner in which participants did the exercise, using the dependent variable `classe` in the training dataset. The `classe` variable is a factor with five classes: A, exercise performed exactly according to the specification; B, throwing the elbows to the front; C, lifting the dumbbell only halfway; D, lowering the dumbbell only halfway; and E, throwing the hips to the front. Class A corresponds to the correct execution of the exercise, while the other four classes correspond to common mistakes.

```{r Data setup, message=FALSE, warning=FALSE, comment=FALSE}

library(readr)
library(tidyverse)
library(nortest)
library(Hmisc)
library(FNN)
library(heplots)
library(knitr)

options(max.print=1000000)
options(scipen=999)

# Un-comment the following two lines to download the raw data.
# download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "pml-training.csv")
# download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "pml-testing.csv")
pml_training <- read_csv("pml-training.csv", guess_max = 19622)
pml_testing <- read_csv("pml-testing.csv", guess_max = 20)

```

```{r Data Wrangling and Exploratory Analysis}

# (Code used in the exploratory phase is commented out here to save processing time.)

# glimpse(pml_training, width = 90)                  # A concise look at all the variables.
# sapply(pml_training, function(x) sum(is.na(x)))    # Count the NAs in every column.
# sapply(pml_training, function(x) summary(x))       # Do a summary of every variable, including a count of NAs.
# sapply(pml_training_no_NA, function(x) summary(x))
# sapply(pml_training_no_NA[, 1:52], function(x) {mean(x) - median(x)}) # Compute (mean - median), but not for classe.
# sapply(pml_training_no_NA[, 1:52], function(x) lillie.test(x)) # Normality tests. (The lillie.test works with large n.)
# pairs(pml_training_no_NA, upper.panel = NULL)      # Pairwise scatterplots of predictors.
# rcorr(as.matrix(pml_training_no_NA[-53]))          # Correlation matrix and significance tests.

# Prepare data based on results of exploratory analyses.

# Delete NA variables.
pml_training_no_NA <- pml_training[, -which(colMeans(is.na(pml_training)) > 0.5)] 
pml_testing_no_NA <- pml_testing[, -which(colMeans(is.na(pml_testing)) > 0.5)]

# Delete irrelevant variables.
pml_training_no_NA <- select(pml_training_no_NA, "roll_belt":"classe")            
pml_testing_no_NA <- select(pml_testing_no_NA, "roll_belt":"magnet_forearm_z")

pml_training_no_NA$classe <- as.factor(pml_training_no_NA$classe) # Convert character variable classe to a factor.
classe <- pml_training_no_NA$classe # Need classe in its own vector for FNN::knn.
pml_training_no_NA_no_classe <- pml_training_no_NA[-53] # Get rid of classe for FNN::knn.
theBoxM <- boxM(pml_training_no_NA_no_classe, pml_training_no_NA$classe) # Test for homogeneity of covariance matrices.

```

Of the 160 variables in the training set, there were many with high occurrence of NA values. The number of NA values was always 19216, close to the total number of observations in the training set (19622), suggestive of a device failure. Those variables were eliminated from the analysis, in addition to several other variables deemed irrelevant to predicting exercise performance. 

This left 52 quantitative predictor variables with no missing values. The Lilliefors test for normality (package [nortest](https://www.rdocumentation.org/packages/nortest/versions/1.0-4)) showed that all 52 predictors were significantly non-normal in distribution. In addition, Box's M-test (package [heplots](https://www.rdocumentation.org/packages/heplots/versions/1.3-5)) indicated extreme heterogeneity of predictor covariance matrices across levels of the dependent variable `classe` (`p < 0.00000000000000022`). Violation of these two assumptions eliminated both linear and quadratic discriminant analysis as choices for classification model building. Logistic regression was also eliminated due to the high degree of multicollinearity in the predictor variables, as indicated by matrices of predictor correlations and p-values (function [rcorr](https://www.rdocumentation.org/packages/Hmisc/versions/4.1-1/topics/rcorr)). 

Given the limitations of the data, we chose k-Nearest Neighbour (KNN) Classification (package [FNN](https://www.rdocumentation.org/packages/FNN/versions/1.1.2.1), function [knn](https://www.rdocumentation.org/packages/FNN/versions/1.1.2.1/topics/knn)) to build our predictive model. KNN is completely nonparametric and makes no assumptions about the distribution of the data. To predict a case from the test data, KNN finds the `k` training observations that are closest to the test case, using [Euclidean distances](https://en.wikipedia.org/wiki/Euclidean_distance) calculated from the predictor variables. The test case is then assigned to the class to which the majority of the `k` nearest neighbors belong.^[James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An Introduction to Statistical Learning. Springer.]


```{r k-nearest neighbors}

# Leave-one-out cross-validation of the training set.
theKNN_CV <- knn.cv(pml_training_no_NA_no_classe, classe, k = 3, prob = TRUE)
kable(table(classe, theKNN_CV), caption = "Table 1. Confusion matrix for k-Nearest Neighbor Classification, 
      using leave-one-out cross-validation.", 
      format = "pandoc")

# Classification of the test set.
theKNN <- FNN::knn(pml_training_no_NA_no_classe, pml_testing_no_NA, classe, k = 3, prob=TRUE)

```

```{r Prediction and Out-of-Sample Error}

# Classification of the test set.
theKNN <- FNN::knn(pml_training_no_NA_no_classe, pml_testing_no_NA, classe, k = 3, prob=TRUE)

```

From Table 1 we can calculate the estimated out-of-sample error rate as `1 - ((5491+3565+3268+3065+3454)/19622) = 0.0397`, or approximately 4%.

> The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

Linear discriminant analysis assumes that predictors are normally distributed and that the different classes have class-specific means and equal variance/covariance matrices.

If it turns out that our damn covariance matrices are not equal, then we have to do quadratic discriminant analysis. But QDA assumes that covariance matrices are equal, which they ain't.


  







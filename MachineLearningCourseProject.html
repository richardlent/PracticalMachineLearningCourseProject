<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Richard A. Lent" />


<title>Predicting Exercise Performance Using Accelerometer Data</title>

<script src="MachineLearningCourseProject_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="MachineLearningCourseProject_files/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="MachineLearningCourseProject_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="MachineLearningCourseProject_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="MachineLearningCourseProject_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="MachineLearningCourseProject_files/navigation-1.1/tabsets.js"></script>
<link href="MachineLearningCourseProject_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="MachineLearningCourseProject_files/highlightjs-9.12.0/highlight.js"></script>
<script src="MachineLearningCourseProject_files/kePrint-0.0.1/kePrint.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Predicting Exercise Performance Using Accelerometer Data</h1>
<h4 class="author"><em>Richard A. Lent</em></h4>

</div>


<p><strong> 6 September 2018 </strong></p>
<p>This machine learning project uses data<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> from accelerometer devices worn on the belt, forearm, arm, and barbell of six participants, who were asked to perform barbell lifts correctly and incorrectly in five different ways. The goal is to predict the manner in which participants did the exercise, using the dependent variable <code>classe</code> in the training dataset. The <code>classe</code> variable is a factor with five classes: A, exercise performed exactly according to the specification; B, throwing the elbows to the front; C, lifting the dumbbell only halfway; D, lowering the dumbbell only halfway; and E, throwing the hips to the front. Class A corresponds to the correct execution of the exercise, while the other four classes correspond to common mistakes.</p>
<p>In the following presentation, R code that produced the analysis is shown alongside explanatory text.</p>
<pre class="r"><code># Data setup

library(readr)
library(tidyverse)
library(nortest)
library(Hmisc)
library(FNN)
library(heplots)
library(knitr)
library(pander)
library(kableExtra)

options(max.print=1000000)
options(scipen=999)

# Un-comment the following lines to download the raw data.
# download.file(&quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;, 
#               &quot;pml-training.csv&quot;)
# download.file(&quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;, 
#               &quot;pml-testing.csv&quot;)
pml_training &lt;- read_csv(&quot;pml-training.csv&quot;, guess_max = 19622)
pml_testing &lt;- read_csv(&quot;pml-testing.csv&quot;, guess_max = 20)</code></pre>
<pre class="r"><code># Data Wrangling, Exploratory Analysis, and Model Building

# (Code used in the exploratory phase is commented out here to save processing time.)

# glimpse(pml_training, width = 90)               # A concise look at all the variables.
# sapply(pml_training, function(x) sum(is.na(x))) # Count the NAs in every column.
# sapply(pml_training, function(x) summary(x))    # Summarize each variable.

# Delete NA variables.
pml_training_no_NA &lt;- pml_training[, -which(colMeans(is.na(pml_training)) &gt; 0.5)] 
pml_testing_no_NA &lt;- pml_testing[, -which(colMeans(is.na(pml_testing)) &gt; 0.5)]

# Delete irrelevant variables.
pml_training_no_NA &lt;- select(pml_training_no_NA, &quot;roll_belt&quot;:&quot;classe&quot;)            
pml_testing_no_NA &lt;- select(pml_testing_no_NA, &quot;roll_belt&quot;:&quot;magnet_forearm_z&quot;)

# sapply(pml_training_no_NA, function(x) summary(x)) # Summarize each variable.
# sapply(pml_training_no_NA[, 1:52], function(x) {mean(x) - median(x)}) # Compute (mean-median).
# sapply(pml_training_no_NA[, 1:52], function(x) lillie.test(x)) # Normality tests. 
# pairs(pml_training_no_NA, upper.panel = NULL)      # Pairwise scatterplots of predictors.
# rcorr(as.matrix(pml_training_no_NA[-53]))          # Correlation matrix and significance tests.

# Convert character variable classe to a factor.
pml_training_no_NA$classe &lt;- as.factor(pml_training_no_NA$classe) 

classe &lt;- pml_training_no_NA$classe # Need classe in its own vector for FNN::knn.
pml_training_no_NA_no_classe &lt;- pml_training_no_NA[-53] # Get rid of classe for FNN::knn.

# Test for homogeneity of covariance matrices.
theBoxM &lt;- boxM(pml_training_no_NA_no_classe, pml_training_no_NA$classe) </code></pre>
<div id="data-wrangling-exploratory-analysis-and-model-building" class="section level3">
<h3>Data Wrangling, Exploratory Analysis, and Model Building</h3>
<p>Of the 160 variables in the training set, there were many with high occurrence of NA values (missing data). The number of NA values was always 19216, close to the total number of observations in the training set (19622), possibly because of a device failure. Those variables were eliminated from the analysis, in addition to several other variables deemed irrelevant to predicting exercise performance.</p>
<p>This left 52 quantitative predictor variables with no missing values. Predictor variables represented various aspects of movement of the body, arm, and barbell as measured by accelerometers worn by the participants. The Lilliefors test for normality (package <a href="https://www.rdocumentation.org/packages/nortest/versions/1.0-4">nortest</a>), plus computing the difference between the mean and median, and viewing of pairwise scatterplots, showed that all 52 predictors were significantly non-normal in distribution. In addition, Box’s M-test (package <a href="https://www.rdocumentation.org/packages/heplots/versions/1.3-5">heplots</a>) indicated extreme heterogeneity of predictor covariance matrices across levels of the dependent variable <code>classe</code> (<code>p &lt; 0.00000000000000022</code>). Violation of these two assumptions eliminated both linear and quadratic discriminant analysis as choices for classification model building. Logistic regression was also eliminated due to the high degree of multicollinearity in the predictor variables, as indicated by matrices of predictor correlations and p-values (function <a href="https://www.rdocumentation.org/packages/Hmisc/versions/4.1-1/topics/rcorr">rcorr</a>).</p>
<p>Given the data limitations, we chose k-Nearest Neighbor Classification (KNN, package <a href="https://www.rdocumentation.org/packages/FNN/versions/1.1.2.1">FNN</a>) to build our predictive engine. KNN is completely nonparametric and makes no assumptions about the distribution of the data, so that it is a “model-free” approach to classification.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> To predict a case from the test data, KNN finds the <code>k</code> training observations that are closest to the test case, using a <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a> measure. The test case is then assigned to the class to which the majority of the <code>k</code> nearest neighbors belong. KNN is an appropriate choice for classification problems in which the data do not meet the assumptions of methods based on linear models. However, because KNN does not fit a model to the training data, it does not tell us which predictors have an important relationship to the dependent variable.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> This aspect of KNN was not considered to be critical as this project was focused on prediction, not interpretation.</p>
<pre class="r"><code># k-Nearest Neighbor Classification and Cross-Validation

# Leave-one-out cross-validation of the training set.
theKNN_CV &lt;- knn.cv(pml_training_no_NA_no_classe, classe, k = 3, prob = TRUE)

tab1 &lt;- table(classe, theKNN_CV)
tab1 %&gt;%
  kable(&quot;html&quot;, caption = &quot;Table 1. Confusion matrix for the k-Nearest 
  Neighbor Classification, using a leave-one-out cross-validation of the training data and
  k = 3 nearest neighbors.&quot;) %&gt;%   
    kable_styling(bootstrap_options = &quot;bordered&quot;, full_width = T, position=&quot;center&quot;) %&gt;% 
    footnote(general = &quot;Exercise categories: A, exercise performed exactly according to the specification; B, throwing the elbows to the front; C, lifting the dumbbell only halfway; D, lowering the dumbbell only halfway; and E, throwing the hips to the front. Class A corresponds to the correct execution of the exercise, while the other four classes correspond to common mistakes.&quot;, general_title = &quot;&quot;)</code></pre>
<table class="table table-bordered" style="margin-left: auto; margin-right: auto;">
<caption>
Table 1. Confusion matrix for the k-Nearest Neighbor Classification, using a leave-one-out cross-validation of the training data and k = 3 nearest neighbors.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
A
</th>
<th style="text-align:right;">
B
</th>
<th style="text-align:right;">
C
</th>
<th style="text-align:right;">
D
</th>
<th style="text-align:right;">
E
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
A
</td>
<td style="text-align:right;">
5491
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
7
</td>
</tr>
<tr>
<td style="text-align:left;">
B
</td>
<td style="text-align:right;">
87
</td>
<td style="text-align:right;">
3565
</td>
<td style="text-align:right;">
76
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
37
</td>
</tr>
<tr>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
68
</td>
<td style="text-align:right;">
3268
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
17
</td>
</tr>
<tr>
<td style="text-align:left;">
D
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
101
</td>
<td style="text-align:right;">
3065
</td>
<td style="text-align:right;">
24
</td>
</tr>
<tr>
<td style="text-align:left;">
E
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
52
</td>
<td style="text-align:right;">
3454
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border: 0;" colspan="100%">
<sup></sup> Exercise categories: A, exercise performed exactly according to the specification; B, throwing the elbows to the front; C, lifting the dumbbell only halfway; D, lowering the dumbbell only halfway; and E, throwing the hips to the front. Class A corresponds to the correct execution of the exercise, while the other four classes correspond to common mistakes.
</td>
</tr>
</tfoot>
</table>
</div>
<div id="k-nearest-neighbor-classification-and-cross-validation" class="section level3">
<h3>k-Nearest Neighbor Classification and Cross-Validation</h3>
<p>Table 1 shows the confusion matrix for the k-Nearest Neighbor Classification, using a leave-one-out cross-validation of the training dataset with <code>k = 3</code> nearest neighbors. Matrix elements on the diagonal show the number of cases that were correctly classified, while off-diagonal elements show misclassifications.</p>
<p>From Table 1 we can calculate the estimated out-of-sample error rate as:</p>
<p><code>1 - ((5491 + 3565 + 3268 + 3065 + 3454) / 19622) = 0.0397</code>,</p>
<p>or approximately 4%.</p>
</div>
<div id="classification-of-the-test-data" class="section level3">
<h3>Classification of the Test Data</h3>
<pre class="r"><code># Classification of the Test Data

theKNN &lt;- FNN::knn(pml_training_no_NA_no_classe, pml_testing_no_NA, classe, k = 3, prob=TRUE)

# tab2 &lt;- t(theKNN[1:length(theKNN)])
# tab2 %&gt;%
#   kable(&quot;html&quot;, caption = &quot;Table 2. Classification of 20 test cases using k-Nearest Neighbor 
#       Classification with k = 3 nearest neighbors.&quot;, col.names = as.character(1:20)) %&gt;%   
#     kable_styling(bootstrap_options = &quot;bordered&quot;, full_width = T, position=&quot;center&quot;)</code></pre>
<p>The same KNN algorithm was used to classify the test dataset, consisting of 20 different test cases. All test cases were classified correctly.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>See <a href="http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har">Human Activity Recognition</a>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Hastie, T., R. Tibshirani, and J. Friedman. 2009. The Elements of Statistical Learning. Second edition. Springer-Verlag New York Inc.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An Introduction to Statistical Learning. Springer-Verlag New York Inc.<a href="#fnref3">↩</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
